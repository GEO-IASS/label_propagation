{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import sys\n",
    "\n",
    "#from pylab import *\n",
    "#%pylab inline\n",
    "#pylab.rcParams['figure.figsize'] = (50, 10)\n",
    "#np.set_printoptions(precision = 17)\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "from cStringIO import StringIO\n",
    "import IPython.display\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# caffe layers\n",
    "caffe_root = '/users/vijay.kumar/caffe/'\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "import caffe\n",
    "from caffe import layers as L\n",
    "\n",
    "# enable gpu\n",
    "caffe.set_mode_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create transformer for the input called 'data'\n",
    "transformer = caffe.io.Transformer({'data': (1,3,224,224)})\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "#transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR # not required.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proto_file = '/users/vijay.kumar/code/person_recognition/models/new/vgg_face_caffe/VGG_FACE_deploy.prototxt'\n",
    "weights = '/users/vijay.kumar/code/person_recognition/models/new/vgg_face_caffe/VGG_FACE.caffemodel'\n",
    "net = caffe.Net(proto_file, weights, caffe.TEST)\n",
    "net.forward()\n",
    "avgImg = [129.1863,104.7624,93.5940]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63188\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "album_gt_path = '/data6/vijay.kumar/person_recognition/annotations/index.txt'\n",
    "image_dir = '/data6/vijay.kumar/person_recognition/data/'\n",
    "count = 0\n",
    "num_examples = sum(1 for line in open(album_gt_path))\n",
    "\n",
    "split_dir = {1:'train',2:'val',3:'test'}\n",
    "features = np.zeros((num_examples, 4096))\n",
    "with open(album_gt_path) as f:\n",
    "    for line in f:        \n",
    "        line_split = line.strip().split(' ')        \n",
    "        img_name = line_split[0] + '_' + line_split[1] + '.jpg'\n",
    "        split_no = int(line_split[7])\n",
    "        if  split_no == 0:\n",
    "            continue\n",
    "        \n",
    "        img_name = image_dir + split_dir[split_no] + '/' + img_name                \n",
    "        label = line_split[6]\n",
    "        image = cv2.imread(img_name)     ## NOTE USING OPENCV TO READ.. # BGR Format\n",
    "        #showarray(image)                \n",
    "        x1 = int(line_split[2])\n",
    "        y1 = int(line_split[3])\n",
    "        w = int(line_split[4])\n",
    "        h = int(line_split[5])\n",
    "                \n",
    "        face = image[y1:y1+h,x1:x1+w,:]\n",
    "        face[0] = face[0] - avgImg[2]\n",
    "        face[1] = face[1] - avgImg[1]\n",
    "        face[2] = face[2] - avgImg[0]\n",
    "    \n",
    "        #showarray(face)                \n",
    "        transformed_image = transformer.preprocess('data', face)                \n",
    "        net.blobs['data'].data[0, ...] = transformed_image\n",
    "        net.forward(start='conv1_1')\n",
    "        features[count] = net.blobs['fc7'].data\n",
    "        count = count + 1        \n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print count\n",
    "        \n",
    "np.savez('/home/vijay.kumar/semi-supervised/pipa_feas',features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
